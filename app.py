import streamlit as st
import google.generativeai as genai
import pandas as pd
import json
import tempfile
import os
import base64
import time
import glob
import re
from st_aggrid import AgGrid, GridOptionsBuilder, JsCode
from io import BytesIO
from pathlib import Path
from functools import lru_cache
from typing import List, Dict, Optional

st.set_page_config(
    layout="wide",
    page_title="Drawing Review",
    page_icon="üèóÔ∏è",
    initial_sidebar_state="expanded"
)

def hide_branding():
    st.markdown("""
        <style>
            div[class^="_profilePreview"] {
                display: none !important;
            }
        </style>
    """, unsafe_allow_html=True)

hide_branding()

RULES_DIR = "rules"
MAX_RETRIES = 3
RETRY_DELAY = 2

@st.cache_resource
def init_genai():
    if "GOOGLE_API_KEY" in st.secrets:
        api_key = st.secrets["GOOGLE_API_KEY"]
    else:
        st.error("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ API –∫–ª—é—á —É secrets.toml")
        st.stop()
    
    try:
        genai.configure(api_key=api_key)
        return True
    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó API: {e}")
        st.stop()

init_genai()

def display_pdf(file_path: str) -> None:
    try:
        with open(file_path, "rb") as f:
            pdf_bytes = f.read()
            base64_pdf = base64.b64encode(pdf_bytes).decode('utf-8')
        
        pdf_display = f'''
            <iframe 
                src="data:application/pdf;base64,{base64_pdf}#toolbar=1&navpanes=0&scrollbar=1" 
                width="100%" 
                height="800" 
                type="application/pdf"
                style="border: none;">
                <p>–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î –≤–±—É–¥–æ–≤–∞–Ω–∏–π –ø–µ—Ä–µ–≥–ª—è–¥ PDF. 
                   <a href="data:application/pdf;base64,{base64_pdf}" download="drawing.pdf">–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —Ñ–∞–π–ª</a>
                </p>
            </iframe>
        '''
        st.markdown(pdf_display, unsafe_allow_html=True)
        
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            st.download_button(
                label="üì• –Ø–∫—â–æ PDF –Ω–µ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î—Ç—å—Å—è - –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —Ñ–∞–π–ª",
                data=pdf_bytes,
                file_name="drawing.pdf",
                mime="application/pdf",
                help="Chrome –º–æ–∂–µ –±–ª–æ–∫—É–≤–∞—Ç–∏ –≤–±—É–¥–æ–≤–∞–Ω–∏–π –ø–µ—Ä–µ–≥–ª—è–¥ PDF"
            )
    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è PDF: {e}")
        try:
            with open(file_path, "rb") as f:
                st.download_button(
                    label="üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ PDF",
                    data=f.read(),
                    file_name="drawing.pdf",
                    mime="application/pdf"
                )
        except:
            st.error("–ù–µ–º–æ–∂–ª–∏–≤–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ñ–∞–π–ª")

def clean_json_text(text: str) -> str:
    text = text.strip()
    if text.startswith("```json"):
        text = text[7:]
    elif text.startswith("```"):
        text = text[3:]
    if text.endswith("```"):
        text = text[:-3]
    return text.strip()

@lru_cache(maxsize=32)
def load_json_file(file_path: str) -> Optional[Dict]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        st.sidebar.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è {Path(file_path).name}: {e}")
        return None

def load_rules_from_json(selected_files: List[str]) -> str:
    combined_rules = ""
    for file_path in selected_files:
        data = load_json_file(file_path)
        if data:
            filename = Path(file_path).name
            combined_rules += f"\n--- SOURCE: {filename} ---\n"
            combined_rules += json.dumps(data, indent=2, ensure_ascii=False)
            combined_rules += "\n"
    return combined_rules

def to_excel(df: pd.DataFrame) -> bytes:
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Analysis Results')
        worksheet = writer.sheets['Analysis Results']
        for idx, col in enumerate(df.columns):
            max_length = max(
                df[col].astype(str).str.len().max(),
                len(col)
            )
            worksheet.column_dimensions[chr(65 + idx)].width = min(max_length + 2, 50)
    return output.getvalue()

def upload_file_with_retry(file_path: str, mime_type: str = "application/pdf"):
    for attempt in range(MAX_RETRIES):
        try:
            uploaded_file = genai.upload_file(file_path, mime_type=mime_type)
            
            timeout = 60
            start_time = time.time()
            while uploaded_file.state.name == "PROCESSING":
                if time.time() - start_time > timeout:
                    raise TimeoutError("–ü–µ—Ä–µ–≤–∏—â–µ–Ω–æ —á–∞—Å –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –æ–±—Ä–æ–±–∫–∏ —Ñ–∞–π–ª—É")
                time.sleep(1)
                uploaded_file = genai.get_file(uploaded_file.name)
            
            if uploaded_file.state.name == "FAILED":
                raise ValueError("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–±—Ä–æ–±–∏—Ç–∏ PDF –Ω–∞ —Å—Ç–æ—Ä–æ–Ω—ñ Google")
            
            return uploaded_file
            
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                st.warning(f"‚ö†Ô∏è –°–ø—Ä–æ–±–∞ {attempt + 1}/{MAX_RETRIES} –Ω–µ –≤–¥–∞–ª–∞—Å—è. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {RETRY_DELAY}—Å...")
                time.sleep(RETRY_DELAY)
            else:
                raise e

def analyze_pdf_drawing(file_path: str, rules_text: str, model_name: str) -> str:
    with st.spinner("üì§ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É..."):
        uploaded_file_ref = upload_file_with_retry(file_path)
    
    prompt = f"""
Role: Lead Quality Control Engineer with expertise in technical documentation standards.

Task: Analyze this technical drawing PDF against the specific Ruleset provided below. 
Identify ALL violations, inconsistencies, and areas of non-compliance.

ACTIVE RULESET (Strictly follow these requirements):
{rules_text}

Instructions:
1. Check each page systematically
2. Identify specific components with issues
3. Provide actionable fix recommendations
4. Assess criticality (High/Medium/Low)

Output Format:
Return ONLY a valid JSON array. No markdown, no explanations.
Example:
[
    {{
        "page": 1,
        "component": "Shaft Detail A",
        "issue": "Missing tolerance specification per ISO 286-1",
        "fix": "Add tolerance class h7 to diameter 25mm",
        "criticality": "High"
    }}
]

If no issues found, return: []
"""
    
    model = genai.GenerativeModel(model_name)
    
    with st.spinner("ü§ñ AI –∞–Ω–∞–ª—ñ–∑—É—î –∫—Ä–µ—Å–ª–µ–Ω–Ω—è..."):
        response = model.generate_content(
            [prompt, uploaded_file_ref],
            generation_config={
                "response_mime_type": "application/json",
                "temperature": 0.1,
            }
        )
    
    try:
        genai.delete_file(uploaded_file_ref.name)
    except:
        pass
    
    return response.text

def parse_csv_value(value_str: str) -> str:
    try:
        cleaned = value_str.strip('"')
        if cleaned.startswith('{"Value":'):
            data = json.loads(cleaned)
            return data.get("Value", "")
        return cleaned
    except:
        return value_str

def load_csv_file(uploaded_file) -> pd.DataFrame:
    try:
        df = pd.read_csv(uploaded_file, sep=';', encoding='utf-8-sig')
        
        for col in df.columns:
            df[col] = df[col].apply(parse_csv_value)
        
        return df
    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è CSV: {e}")
        return None

def extract_date_from_filename(filename: str) -> str:
    match = re.search(r'(\d{4}[-_]\d{2}[-_]\d{2})', filename)
    if match:
        return match.group(1)
    return ""

def compare_csv_files(df1: pd.DataFrame, df2: pd.DataFrame, file1_name: str, file2_name: str) -> Dict:
    date1 = extract_date_from_filename(file1_name)
    date2 = extract_date_from_filename(file2_name)
    
    if date2 > date1:
        old_df, new_df = df1, df2
        old_name, new_name = file1_name, file2_name
    else:
        old_df, new_df = df2, df1
        old_name, new_name = file2_name, file1_name
    
    old_paths = set(old_df['full_path'].values)
    new_paths = set(new_df['full_path'].values)
    
    added_files = new_paths - old_paths
    deleted_files = old_paths - new_paths
    common_files = old_paths & new_paths
    
    modified_files = []
    for path in common_files:
        old_row = old_df[old_df['full_path'] == path].iloc[0]
        new_row = new_df[new_df['full_path'] == path].iloc[0]
        
        if 'last_modif' in old_df.columns and 'last_modif' in new_df.columns:
            if old_row['last_modif'] != new_row['last_modif']:
                modified_files.append(path)
    
    return {
        'added': list(added_files),
        'deleted': list(deleted_files),
        'modified': modified_files,
        'old_name': old_name,
        'new_name': new_name
    }

if 'analysis_df' not in st.session_state:
    st.session_state.analysis_df = None
if 'last_uploaded_filename' not in st.session_state:
    st.session_state.last_uploaded_filename = None
if 'selected_model' not in st.session_state:
    st.session_state.selected_model = "gemini-1.5-flash"

with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è")
    
    @st.cache_data(ttl=3600)
    def get_available_models():
        try:
            models = genai.list_models()
            available = {}
            for model in models:
                if 'generateContent' in model.supported_generation_methods:
                    model_name = model.name.replace('models/', '')
                    if 'gemini-3' in model_name.lower():
                        display_name = f"üî• {model_name} (Gemini 3)"
                    elif 'gemini-2.5' in model_name.lower():
                        display_name = f"‚ö° {model_name} (Gemini 2.5)"
                    elif 'gemini-2.0' in model_name.lower():
                        display_name = f"üí® {model_name} (Gemini 2.0)"
                    elif 'gemini-1.5-pro' in model_name.lower():
                        display_name = f"üéØ {model_name} (Gemini 1.5 Pro)"
                    elif 'gemini-1.5-flash' in model_name.lower():
                        display_name = f"‚ö° {model_name} (Gemini 1.5 Flash)"
                    else:
                        display_name = model_name
                    
                    available[model_name] = display_name
            return available
        except Exception as e:
            st.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π: {e}")
            return {"gemini-1.5-flash": "‚ö° gemini-1.5-flash (Gemini 1.5 Flash)"}
    
    available_models = get_available_models()
    
    if available_models:
        model_options = list(available_models.keys())
        
        default_index = 0
        if st.session_state.selected_model in model_options:
            default_index = model_options.index(st.session_state.selected_model)
        elif 'gemini-2.5-flash' in model_options:
            default_index = model_options.index('gemini-2.5-flash')
        elif 'gemini-1.5-flash' in model_options:
            default_index = model_options.index('gemini-1.5-flash')
        
        selected_model = st.selectbox(
            "ü§ñ –ú–æ–¥–µ–ª—å AI:",
            options=model_options,
            format_func=lambda x: available_models[x],
            index=default_index,
            key="model_selector",
            help="–û–±–µ—Ä—ñ—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∫—Ä–µ—Å–ª–µ–Ω—å"
        )
        st.session_state.selected_model = selected_model
        
        st.caption(f"üìä –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(available_models)}")
    else:
        st.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—ñ")
        st.session_state.selected_model = "gemini-1.5-flash"
    
    st.divider()
    st.header("üìö –ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –°—Ç–∞–Ω–¥–∞—Ä—Ç—ñ–≤")
    
    st.caption("–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ —Ñ–∞–π–ª–∏:")
    uploaded_json_files = st.file_uploader(
        "–û–±–µ—Ä—ñ—Ç—å JSON —Ñ–∞–π–ª–∏",
        type=["json"],
        accept_multiple_files=True,
        help="–î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø—Ä–∞–≤–∏–ª–∞, —è–∫—ñ –¥–æ–ø–æ–≤–Ω—è—Ç—å —Ñ–∞–π–ª–∏ –∑ –ø–∞–ø–∫–∏ rules/",
        key="json_uploader"
    )
    
    if uploaded_json_files:
        if 'uploaded_rules_files' not in st.session_state:
            st.session_state.uploaded_rules_files = {}
        
        for uploaded_file in uploaded_json_files:
            st.session_state.uploaded_rules_files[uploaded_file.name] = uploaded_file.getvalue()
    
    st.divider()
    
    all_files = {}
    
    Path(RULES_DIR).mkdir(exist_ok=True)
    local_json_files = list(Path(RULES_DIR).glob("*.json"))
    for file_path in local_json_files:
        all_files[f"local:{file_path.name}"] = {
            "name": file_path.name,
            "path": str(file_path),
            "source": "üìÅ –õ–æ–∫–∞–ª—å–Ω—ñ",
            "default": True
        }
    
    if 'uploaded_rules_files' in st.session_state:
        for file_name, file_content in st.session_state.uploaded_rules_files.items():
            temp_path = Path(tempfile.gettempdir()) / f"uploaded_{file_name}"
            temp_path.write_bytes(file_content)
            
            all_files[f"upload:{file_name}"] = {
                "name": file_name,
                "path": str(temp_path),
                "source": "‚òÅÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ",
                "default": False
            }
    
    selected_files = []
    
    if all_files:
        st.caption("–û–±–µ—Ä—ñ—Ç—å —Ñ–∞–π–ª–∏ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:")
        
        local_files = {k: v for k, v in all_files.items() if v["source"] == "üìÅ –õ–æ–∫–∞–ª—å–Ω—ñ"}
        uploaded_files = {k: v for k, v in all_files.items() if v["source"] == "‚òÅÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ"}
        
        if local_files:
            st.markdown("**üìÅ –§–∞–π–ª–∏ –∑ –ø–∞–ø–∫–∏ rules/ (–≤–∫–ª—é—á–µ–Ω—ñ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º):**")
            for key, file_info in local_files.items():
                if st.checkbox(
                    f"{file_info['name']}",
                    value=file_info['default'],
                    key=f"cb_{key}",
                    help=f"–î–∂–µ—Ä–µ–ª–æ: {file_info['source']}"
                ):
                    selected_files.append(file_info['path'])
        
        if uploaded_files:
            st.markdown("**‚òÅÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ —Ñ–∞–π–ª–∏:**")
            for key, file_info in uploaded_files.items():
                if st.checkbox(
                    f"{file_info['name']}",
                    value=file_info['default'],
                    key=f"cb_{key}",
                    help=f"–î–∂–µ—Ä–µ–ª–æ: {file_info['source']}"
                ):
                    selected_files.append(file_info['path'])
    else:
        st.info("üìÇ –§–∞–π–ª—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –î–æ–¥–∞–π—Ç–µ JSON –≤ –ø–∞–ø–∫—É rules/ –∞–±–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —á–µ—Ä–µ–∑ —Ñ–æ—Ä–º—É –≤–∏—â–µ")
    
    if 'uploaded_rules_files' in st.session_state and st.session_state.uploaded_rules_files:
        if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ —Ñ–∞–π–ª–∏"):
            st.session_state.uploaded_rules_files = {}
            st.rerun()

    st.divider()
    
    if selected_files:
        st.subheader("üëÄ –ê–∫—Ç–∏–≤–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞")
        with st.expander(f"üîç {len(selected_files)} —Ñ–∞–π–ª(—ñ–≤) –≤–∏–±—Ä–∞–Ω–æ"):
            active_rules_preview = load_rules_from_json(selected_files)
            st.code(active_rules_preview, language="json")
        
        total_chars = len(active_rules_preview)
        st.caption(f"üìä –†–æ–∑–º—ñ—Ä –ø—Ä–æ–º–ø—Ç—É: ~{total_chars:,} —Å–∏–º–≤–æ–ª—ñ–≤")
    else:
        st.warning("‚ö†Ô∏è –ù–µ –≤–∏–±—Ä–∞–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—É!")

main_tab1, main_tab2 = st.tabs(["üîç Check", "‚öñÔ∏è Compare"])

with main_tab1:
    uploaded_file = st.file_uploader(
        "üìé –ó–∞–≤–∞–Ω—Ç–∞–∂ PDF –∫—Ä–µ—Å–ª–µ–Ω–Ω—è",
        type=["pdf"],
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å Streamlit",
        key="pdf_uploader"
    )

    if uploaded_file and uploaded_file.name != st.session_state.last_uploaded_filename:
        st.session_state.analysis_df = None
        st.session_state.last_uploaded_filename = uploaded_file.name

    if uploaded_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        tab1, tab2 = st.tabs(["üìÑ –ü–µ—Ä–µ–≥–ª—è–¥", "ü§ñ –ê–Ω–∞–ª—ñ–∑"])
        
        with tab1:
            display_pdf(tmp_file_path)
        
        with tab2:
            st.subheader("üîç –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏")
            
            if len(selected_files) == 0:
                st.warning("‚ö†Ô∏è –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–∏–±–µ—Ä–∏ —Ö–æ—á–∞ –± –æ–¥–∏–Ω —Ñ–∞–π–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ñ–≤ —É –º–µ–Ω—é –∑–ª—ñ–≤–∞!")
                st.info("üëà –í—ñ–¥–∫—Ä–∏–π —Å–∞–π–¥–±–∞—Ä —Ç–∞ –≤—ñ–¥–º—ñ—Ç—å –ø–æ—Ç—Ä—ñ–±–Ω—ñ JSON —Ñ–∞–π–ª–∏ –≤ —Ä–æ–∑–¥—ñ–ª—ñ 'üìö –ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –°—Ç–∞–Ω–¥–∞—Ä—Ç—ñ–≤'")
            
            col1, col2 = st.columns([3, 1])
            with col1:
                analyze_btn = st.button(
                    "üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É",
                    type="primary",
                    use_container_width=True,
                    key="analyze_button"
                )
            with col2:
                if st.session_state.analysis_df is not None:
                    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç–∏", use_container_width=True, key="clear_button"):
                        st.session_state.analysis_df = None
                        st.rerun()
            
            if analyze_btn:
                if len(selected_files) == 0:
                    st.error("‚ùå –ù–µ–º–æ–∂–ª–∏–≤–æ –∑–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –±–µ–∑ –≤–∏–±—Ä–∞–Ω–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ñ–≤!")
                else:
                    st.session_state.analysis_df = None
                    
                    try:
                        final_rules_text = load_rules_from_json(selected_files)
                        raw_response = analyze_pdf_drawing(
                            tmp_file_path,
                            final_rules_text,
                            st.session_state.selected_model
                        )
                        
                        json_response = clean_json_text(raw_response)
                        data = json.loads(json_response)

                        if not data:
                            st.success("‚úÖ –ß—É–¥–æ–≤–æ! AI –Ω–µ –∑–Ω–∞–π—à–æ–≤ –∂–æ–¥–Ω–∏—Ö –ø–æ—Ä—É—à–µ–Ω—å –≤–∏–±—Ä–∞–Ω–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ñ–≤.")
                        else:
                            st.session_state.analysis_df = pd.DataFrame(data)
                            st.success(f"‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ó–Ω–∞–π–¥–µ–Ω–æ {len(data)} –Ω–µ–≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç–µ–π.")

                    except json.JSONDecodeError as e:
                        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON: {e}")
                        with st.expander("üêõ Debug Info"):
                            st.code(json_response if 'json_response' in locals() else raw_response)
                    except Exception as e:
                        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: {e}")
                        with st.expander("üêõ –î–µ—Ç–∞–ª—ñ –ø–æ–º–∏–ª–∫–∏"):
                            st.exception(e)

            if st.session_state.analysis_df is not None:
                df = st.session_state.analysis_df
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("–í—Å—å–æ–≥–æ –ø—Ä–æ–±–ª–µ–º", len(df))
                with col2:
                    high_count = len(df[df['criticality'] == 'High']) if 'criticality' in df.columns else 0
                    st.metric("–ö—Ä–∏—Ç–∏—á–Ω–∏—Ö", high_count, delta=None, delta_color="inverse")
                with col3:
                    unique_pages = df['page'].nunique() if 'page' in df.columns else 0
                    st.metric("–°—Ç–æ—Ä—ñ–Ω–æ–∫ –∑ –ø—Ä–æ–±–ª–µ–º–∞–º–∏", unique_pages)
                
                st.divider()
                
                gb = GridOptionsBuilder.from_dataframe(df)
                gb.configure_default_column(
                    resizable=True,
                    wrapText=True,
                    autoHeight=True,
                    sortable=True,
                    filter=True
                )
                
                if 'issue' in df.columns:
                    gb.configure_column("issue", width=400)
                if 'fix' in df.columns:
                    gb.configure_column("fix", width=400)
                if 'component' in df.columns:
                    gb.configure_column("component", width=200)
                if 'page' in df.columns:
                    gb.configure_column("page", width=80)
                if 'criticality' in df.columns:
                    gb.configure_column("criticality", width=120)
                    
                    jscode = JsCode("""
                    function(params) {
                        if (params.value === 'High') {
                            return {'color': 'white', 'backgroundColor': '#dc3545', 'fontWeight': 'bold'};
                        }
                        if (params.value === 'Medium') {
                            return {'color': 'black', 'backgroundColor': '#ffc107'};
                        }
                        if (params.value === 'Low') {
                            return {'color': 'black', 'backgroundColor': '#28a745', 'color': 'white'};
                        }
                        return {'color': 'black', 'backgroundColor': 'white'};
                    };
                    """)
                    gb.configure_column("criticality", cellStyle=jscode)
                
                gb.configure_pagination(paginationAutoPageSize=False, paginationPageSize=20)
                gb.configure_selection(selection_mode="multiple", use_checkbox=True)
                
                gridOptions = gb.build()

                grid_response = AgGrid(
                    df,
                    gridOptions=gridOptions,
                    height=500,
                    allow_unsafe_jscode=True,
                    enable_enterprise_modules=False,
                    theme="streamlit",
                    key='analysis_grid',
                    reload_data=False
                )

                st.divider()
                col1, col2 = st.columns(2)
                
                with col1:
                    excel_data = to_excel(df)
                    st.download_button(
                        label="üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ Excel",
                        data=excel_data,
                        file_name=f"analysis_{Path(uploaded_file.name).stem}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True
                    )
                
                with col2:
                    csv_data = df.to_csv(index=False).encode('utf-8-sig')
                    st.download_button(
                        label="üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV",
                        data=csv_data,
                        file_name=f"analysis_{Path(uploaded_file.name).stem}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )

        try:
            os.unlink(tmp_file_path)
        except:
            pass

    else:
        st.info("üëÜ –ó–∞–≤–∞–Ω—Ç–∞–∂ PDF –∫—Ä–µ—Å–ª–µ–Ω–Ω—è, —â–æ–± –ø–æ—á–∞—Ç–∏ —Ä–æ–±–æ—Ç—É")
        st.session_state.analysis_df = None
        st.session_state.last_uploaded_filename = None

with main_tab2:
    st.subheader("‚öñÔ∏è –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤ CSV")
    st.caption("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ –¥–≤–∞ CSV —Ñ–∞–π–ª–∏ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑–º—ñ–Ω —É —Ñ–∞–π–ª–æ–≤—ñ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ñ")
    
    csv_files = st.file_uploader(
        "üìÇ –û–±–µ—Ä—ñ—Ç—å 2 CSV —Ñ–∞–π–ª–∏ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è",
        type=["csv"],
        accept_multiple_files=True,
        key="csv_files",
    )
    
    if csv_files and len(csv_files) == 2:
        csv_file1, csv_file2 = csv_files[0], csv_files[1]
        
        with st.spinner("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤..."):
                df1 = load_csv_file(csv_file1)
                df2 = load_csv_file(csv_file2)
                
                if df1 is not None and df2 is not None:
                    comparison = compare_csv_files(df1, df2, csv_file1.name, csv_file2.name)
                    
                    st.success("‚úÖ –§–∞–π–ª–∏ —É—Å–ø—ñ—à–Ω–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ!")
                    
                    info_col1, info_col2 = st.columns(2)
                    with info_col1:
                        st.info(f"üìÖ **–°—Ç–∞—Ä—ñ—à–∏–π —Ñ–∞–π–ª:** {comparison['old_name']}")
                    with info_col2:
                        st.info(f"üìÖ **–ù–æ–≤—ñ—à–∏–π —Ñ–∞–π–ª:** {comparison['new_name']}")
                
                st.divider()
                
                metric_col1, metric_col2, metric_col3 = st.columns(3)
                with metric_col1:
                    st.metric("üÜï –ù–æ–≤—ñ —Ñ–∞–π–ª–∏", len(comparison['added']))
                with metric_col2:
                    st.metric("‚úèÔ∏è –ó–º—ñ–Ω–µ–Ω—ñ —Ñ–∞–π–ª–∏", len(comparison['modified']))
                with metric_col3:
                    st.metric("üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω—ñ —Ñ–∞–π–ª–∏", len(comparison['deleted']))
                
                st.divider()
                
                result_tab1, result_tab2, result_tab3 = st.tabs([
                    f"üÜï –ù–æ–≤—ñ —Ñ–∞–π–ª–∏ ({len(comparison['added'])})",
                    f"‚úèÔ∏è –ó–º—ñ–Ω–µ–Ω—ñ —Ñ–∞–π–ª–∏ ({len(comparison['modified'])})",
                    f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω—ñ —Ñ–∞–π–ª–∏ ({len(comparison['deleted'])})"
                ])
                
                with result_tab1:
                    if comparison['added']:
                        st.subheader("–°–ø–∏—Å–æ–∫ –Ω–æ–≤–∏—Ö —Ñ–∞–π–ª—ñ–≤")
                        for idx, file_path in enumerate(sorted(comparison['added']), 1):
                            st.text(f"{idx}.")
                            st.code(file_path.replace('/', '\\'), language=None)
                    else:
                        st.info("‚úÖ –ù–æ–≤–∏—Ö —Ñ–∞–π–ª—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                
                with result_tab2:
                    if comparison['modified']:
                        st.subheader("–°–ø–∏—Å–æ–∫ –∑–º—ñ–Ω–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤")
                        for idx, file_path in enumerate(sorted(comparison['modified']), 1):
                            st.text(f"{idx}.")
                            st.code(file_path.replace('/', '\\'), language=None)
                    else:
                        st.info("‚úÖ –ó–º—ñ–Ω–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                
                with result_tab3:
                    if comparison['deleted']:
                        st.subheader("–°–ø–∏—Å–æ–∫ –≤–∏–¥–∞–ª–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤")
                        for idx, file_path in enumerate(sorted(comparison['deleted']), 1):
                            st.text(f"{idx}.")
                            st.code(file_path.replace('/', '\\'), language=None)
                    else:
                        st.info("‚úÖ –í–∏–¥–∞–ª–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
    elif csv_files and len(csv_files) != 2:
        st.warning(f"‚ö†Ô∏è –ü–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–±—Ä–∞—Ç–∏ —Ä—ñ–≤–Ω–æ 2 —Ñ–∞–π–ª–∏. –ó–∞—Ä–∞–∑ –≤–∏–±—Ä–∞–Ω–æ: {len(csv_files)}")
    else:
        st.info("üëÜ –û–±–µ—Ä—ñ—Ç—å 2 CSV —Ñ–∞–π–ª–∏ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è (–º–æ–∂–Ω–∞ –≤–∏–±—Ä–∞—Ç–∏ –æ–±–∏–¥–≤–∞ –æ–¥–Ω–æ—á–∞—Å–Ω–æ)")

st.divider()
